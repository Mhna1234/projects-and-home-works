{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mhna1234/projects-and-home-works/blob/main/IR_RM3_DMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Lc2FbSK6vL",
        "outputId": "a8457063-084c-4011-d339-861410472f14"
      },
      "outputs": [
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: '/Users/USER/Desktop/feedback_docs'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-6-d5f9736705b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m# Step 1: Load feedback documents and collection stats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mfeedback_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_documents_or_queries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/USER/Desktop/feedback_docs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[0mtfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_collection_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/USER/Desktop/gov2_collection_stats.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-d5f9736705b7>\u001b[0m in \u001b[0;36mparse_documents_or_queries\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     '''\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdoc_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mdocs_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocs_tfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/Users/USER/Desktop/feedback_docs'"
          ]
        }
      ],
      "source": [
        "import xml.etree.cElementTree as ET\n",
        "\n",
        "def output_xml(modified_queries, out_path):\n",
        "    '''\n",
        "    Writes your modified query models to an XML indri query file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modified_queries : dict[str, dict[str,float]]\n",
        "        A dictionary that maps a query id to the modified query model.\n",
        "        A model is a dictionary that maps terms to their weight.\n",
        "    out_path : str\n",
        "        Path of the output.\n",
        "    '''\n",
        "    params = ET.Element('parameters')\n",
        "    for qid, q_m in modified_queries.iteritems():\n",
        "        q = ET.SubElement(params, 'query')\n",
        "        number = ET.SubElement(q, 'number')\n",
        "        number.text = qid\n",
        "        text = ET.SubElement(q, 'text')\n",
        "        text.text = _get_indri_query(q_m)\n",
        "    tree = ET.ElementTree(params)\n",
        "    tree.write(out_path, encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def parse_collection_stats(input_file):\n",
        "    '''\n",
        "    Parses the collection stats file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_file : str\n",
        "        The path to the corpus file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tfs : dict[str,int]\n",
        "        Mapping containing the collection frequency of each term.\n",
        "    total_terms : int\n",
        "        The number of total terms in the corpus.\n",
        "    '''\n",
        "    with open(input_file) as corpus_file:\n",
        "        total_terms = 0\n",
        "        tfs = {}\n",
        "        for line in corpus_file.readlines():\n",
        "            if ',' in line:\n",
        "                term, val = line.split(',')\n",
        "                tfs[term] = int(val)\n",
        "    total_terms = tfs[\"TOTAL\"]\n",
        "    del tfs[\"TOTAL\"]\n",
        "    return tfs, total_terms\n",
        "\n",
        "\n",
        "def parse_documents_or_queries(file_path):\n",
        "    '''\n",
        "    Parses a TSV file that contains documents or queries count.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_path : str\n",
        "        Path to the TSV file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    docs_tfs : dict[str, dict[str,int]]\n",
        "        Maps a document id to a dictionary that maps every term to its term frequency in the document.\n",
        "    docs_len : dict[str,int]\n",
        "        A dictionary that maps a document id to the length of that document.\n",
        "    '''\n",
        "    with open(file_path) as doc_file:\n",
        "        docs_len, docs_tfs = {}, {}\n",
        "        for line in doc_file.readlines():\n",
        "            line_arr = line.split('\\t')\n",
        "            if len(line_arr) < 2:\n",
        "                continue\n",
        "            doc_id = line_arr[0]\n",
        "            docs_tfs[doc_id] = _process_text_dict(line_arr[1])\n",
        "            docs_len[doc_id] = sum(docs_tfs[doc_id].values())\n",
        "        return docs_tfs, docs_len\n",
        "\n",
        "def _process_text_dict(dict_str):\n",
        "    res = {}\n",
        "    pairs = dict_str.split(',')\n",
        "    for pair in pairs:\n",
        "        if ':' in pair:\n",
        "            key, val = pair.split(':')\n",
        "            res[key] = int(val)\n",
        "    return res\n",
        "\n",
        "def _get_indri_query(q_m):\n",
        "    val_strings = ['{0} \"{1}\"'.format(weight, term) for term, weight in q_m.iteritems()]\n",
        "    return \"#weight(\" + \" \".join(val_strings) + \")\"\n",
        "\n",
        "# Step 1: Load feedback documents and collection stats\n",
        "#query_ids=[\"711.tsv\",\"710.tsv\",\"709.tsv\".\"708.tsv\",\"707.tsv\",\"706.tsv\",\"705.tsv\",\"704.tsv\",\"702.tsv\",\"701.tsv\"]\n",
        "feedback_docs, docs_len = parse_documents_or_queries(\"/home/student/feedback_docs/711.tsv\")\n",
        "tfs, total_terms = parse_collection_stats(\"/home/student/gov2_collection_stats.csv\")\n",
        "\n",
        "# Step 2: Calculate RM3 relevance model\n",
        "def calculate_rm3(feedback_docs, tfs, total_terms, original_query, top_k=25, original_query_weight=0.5):\n",
        "    modified_queries = {}\n",
        "    for qid, docs in feedback_docs.items():\n",
        "        # Calculate p(w|R)\n",
        "        term_scores = {}\n",
        "        for doc_id, doc_terms in docs.items():\n",
        "            for term, count in doc_terms.items():\n",
        "                if term not in term_scores:\n",
        "                    term_scores[term] = 0\n",
        "                term_scores[term] += count / docs_len[doc_id]  # TF in document\n",
        "\n",
        "        # Normalize and select top_k terms\n",
        "        term_probs = {term: score / total_terms for term, score in term_scores.items()}\n",
        "        top_terms = sorted(term_probs.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "        # Combine with original query\n",
        "        combined_query = {term: original_query_weight * original_query.get(term, 0) for term in original_query}\n",
        "        for term, score in top_terms:\n",
        "            combined_query[term] = combined_query.get(term, 0) + (1 - original_query_weight) * score\n",
        "\n",
        "        modified_queries[qid] = combined_query\n",
        "\n",
        "    return modified_queries\n",
        "\n",
        "# Example usage:\n",
        "original_query = {'term1': 1.0, 'term2': 1.0}  # Replace with actual query terms\n",
        "rm3_queries = calculate_rm3(feedback_docs, tfs, total_terms, original_query)\n",
        "output_xml(rm3_queries, \"rm3.xml\")\n",
        "\n",
        "def calculate_dmm(feedback_docs, tfs, total_terms, original_query, lambda_param=0.1, delta=0.1, top_k=25, original_query_weight=0.5):\n",
        "    modified_queries = {}\n",
        "    for qid, docs in feedback_docs.items():\n",
        "        term_scores = {}\n",
        "        for doc_id, doc_terms in docs.items():\n",
        "            for term, count in doc_terms.items():\n",
        "                if term not in term_scores:\n",
        "                    term_scores[term] = 0\n",
        "                # Apply DMM calculation\n",
        "                term_scores[term] += (count + delta) / (docs_len[doc_id] + delta * len(tfs))\n",
        "\n",
        "        # Normalize and select top_k terms\n",
        "        term_probs = {term: score / total_terms for term, score in term_scores.items()}\n",
        "        top_terms = sorted(term_probs.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "        # Combine with original query\n",
        "        combined_query = {term: original_query_weight * original_query.get(term, 0) for term in original_query}\n",
        "        for term, score in top_terms:\n",
        "            combined_query[term] = combined_query.get(term, 0) + (1 - original_query_weight) * score\n",
        "\n",
        "        modified_queries[qid] = combined_query\n",
        "\n",
        "    return modified_queries\n",
        "\n",
        "# Example usage:\n",
        "dmm_queries = calculate_dmm(feedback_docs, tfs, total_terms, original_query)\n",
        "output_xml(dmm_queries, \"dmm.xml\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ3iXddyK6vO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}